{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a78ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive if using Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    USING_COLAB = True\n",
    "except:\n",
    "    USING_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from datasets import load_dataset\n",
    "import glob\n",
    "import json\n",
    "import concurrent.futures\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf9008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_COLAB:\n",
    "    _METADATA_PATH = \"/content/drive/My Drive/GTSI/Codigos_investigadores/supervised_new_data/dataset/hupd_metadata_2022-02-22.feather\"\n",
    "    path_dataset = \"/content/drive/My Drive/GTSI/Codigos_investigadores/supervised_new_data/dataset/\"\n",
    "else:\n",
    "    # _METADATA_PATH = \"./dataset/hupd_metadata_2022-02-22.feather\"\n",
    "    _METADATA_PATH = \"./dataset/hupd_metadata_2022-02-22.feather\"\n",
    "    path_dataset = \"./dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f00e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(_METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41a6886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4518254, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a6fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['application_number', 'filing_date', 'application_invention_type',\n",
       "       'examiner_full_name', 'examiner_art_unit', 'uspc_class',\n",
       "       'uspc_subclass', 'confirm_number', 'atty_docket_number',\n",
       "       'appl_status_desc', 'appl_status_date', 'file_location',\n",
       "       'file_location_date', 'earliest_pgpub_number', 'earliest_pgpub_date',\n",
       "       'wipo_pub_number', 'wipo_pub_date', 'patent_number',\n",
       "       'patent_issue_date', 'invention_title', 'small_entity_indicator',\n",
       "       'aia_first_to_file', 'publication_number', 'date_application_produced',\n",
       "       'date_application_published', 'main_cpc_label', 'cpc_labels',\n",
       "       'main_ipcr_label', 'ipcr_labels', 'foreign', 'continuation', 'decision',\n",
       "       'decision_as_of_2020'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af70e515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "ACCEPTED         1827571\n",
       "REJECTED         1074665\n",
       "CONT-ACCEPTED     653519\n",
       "PENDING           486181\n",
       "CONT-REJECTED     286937\n",
       "CONT-PENDING      189381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"decision\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ff7c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_number</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>application_invention_type</th>\n",
       "      <th>examiner_full_name</th>\n",
       "      <th>examiner_art_unit</th>\n",
       "      <th>uspc_class</th>\n",
       "      <th>uspc_subclass</th>\n",
       "      <th>confirm_number</th>\n",
       "      <th>atty_docket_number</th>\n",
       "      <th>appl_status_desc</th>\n",
       "      <th>appl_status_date</th>\n",
       "      <th>file_location</th>\n",
       "      <th>file_location_date</th>\n",
       "      <th>earliest_pgpub_number</th>\n",
       "      <th>earliest_pgpub_date</th>\n",
       "      <th>wipo_pub_number</th>\n",
       "      <th>wipo_pub_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_issue_date</th>\n",
       "      <th>invention_title</th>\n",
       "      <th>small_entity_indicator</th>\n",
       "      <th>aia_first_to_file</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>date_application_produced</th>\n",
       "      <th>date_application_published</th>\n",
       "      <th>main_cpc_label</th>\n",
       "      <th>cpc_labels</th>\n",
       "      <th>main_ipcr_label</th>\n",
       "      <th>ipcr_labels</th>\n",
       "      <th>foreign</th>\n",
       "      <th>continuation</th>\n",
       "      <th>decision</th>\n",
       "      <th>decision_as_of_2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10018320</td>\n",
       "      <td>2004-06-29</td>\n",
       "      <td>Utility</td>\n",
       "      <td>MITCHELL, LAURA MCGILLEM</td>\n",
       "      <td>1636</td>\n",
       "      <td>435</td>\n",
       "      <td>007400</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>01-1637</td>\n",
       "      <td>Abandoned -- Failure to Respond to an Office A...</td>\n",
       "      <td>2006-07-10</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>NaT</td>\n",
       "      <td>US20050130116A1</td>\n",
       "      <td>2005-06-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Stable expression of polymorphous forms of hum...</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>false</td>\n",
       "      <td>US20050130116A1-20050616</td>\n",
       "      <td>2005-06-01</td>\n",
       "      <td>2005-06-16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>REJECTED</td>\n",
       "      <td>REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10018639</td>\n",
       "      <td>2004-03-15</td>\n",
       "      <td>Utility</td>\n",
       "      <td>FOX, JOHN C</td>\n",
       "      <td>3753</td>\n",
       "      <td>137</td>\n",
       "      <td>884000</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>442-134 PCT/US</td>\n",
       "      <td>Abandoned -- Failure to Respond to an Office A...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>NaT</td>\n",
       "      <td>US20050081931A1</td>\n",
       "      <td>2005-04-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Connecting device for fluids</td>\n",
       "      <td>UNDISCOUNTED</td>\n",
       "      <td>false</td>\n",
       "      <td>US20050081931A1-20050421</td>\n",
       "      <td>2005-04-06</td>\n",
       "      <td>2005-04-21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>REJECTED</td>\n",
       "      <td>REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10048553</td>\n",
       "      <td>2004-10-18</td>\n",
       "      <td>Utility</td>\n",
       "      <td>SAUCIER, SANDRA E</td>\n",
       "      <td>1651</td>\n",
       "      <td>435</td>\n",
       "      <td>280000</td>\n",
       "      <td>4574.0</td>\n",
       "      <td>21581/0286</td>\n",
       "      <td>Patent Expired Due to NonPayment of Maintenanc...</td>\n",
       "      <td>2010-09-20</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>NaT</td>\n",
       "      <td>US20050080277A1</td>\n",
       "      <td>2005-04-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7094594</td>\n",
       "      <td>2006-08-22</td>\n",
       "      <td>PROCESS FOR PREPARING OPTICALLY ACTIVE 2-[6-(H...</td>\n",
       "      <td>UNDISCOUNTED</td>\n",
       "      <td>false</td>\n",
       "      <td>US20050080277A1-20050414</td>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>2005-04-14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>ACCEPTED</td>\n",
       "      <td>ACCEPTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10048576</td>\n",
       "      <td>2005-03-28</td>\n",
       "      <td>Utility</td>\n",
       "      <td>FRANCIS, FAYE</td>\n",
       "      <td>3725</td>\n",
       "      <td>241</td>\n",
       "      <td>001000</td>\n",
       "      <td>7991.0</td>\n",
       "      <td>020065</td>\n",
       "      <td>Patent Expired Due to NonPayment of Maintenanc...</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>NaT</td>\n",
       "      <td>US20050242216A1</td>\n",
       "      <td>2005-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7229033</td>\n",
       "      <td>2007-06-12</td>\n",
       "      <td>METHOD FOR WORKING AND PROCESSING MATERIALS</td>\n",
       "      <td>UNDISCOUNTED</td>\n",
       "      <td>false</td>\n",
       "      <td>US20050242216A1-20051103</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>2005-11-03</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>ACCEPTED</td>\n",
       "      <td>ACCEPTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10049016</td>\n",
       "      <td>2004-06-08</td>\n",
       "      <td>Utility</td>\n",
       "      <td>LE, MICHAEL</td>\n",
       "      <td>2163</td>\n",
       "      <td>707</td>\n",
       "      <td>100000</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>3113.2.1.1</td>\n",
       "      <td>Patented Case</td>\n",
       "      <td>2007-06-13</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "      <td>NaT</td>\n",
       "      <td>US20050060273A1</td>\n",
       "      <td>2005-03-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7240062</td>\n",
       "      <td>2007-07-03</td>\n",
       "      <td>SYSTEM AND METHOD FOR CREATING A SEARCHABLE WO...</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>false</td>\n",
       "      <td>US20050060273A1-20050317</td>\n",
       "      <td>2005-03-03</td>\n",
       "      <td>2005-03-17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>ACCEPTED</td>\n",
       "      <td>ACCEPTED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  application_number filing_date application_invention_type  \\\n",
       "0           10018320  2004-06-29                    Utility   \n",
       "1           10018639  2004-03-15                    Utility   \n",
       "2           10048553  2004-10-18                    Utility   \n",
       "3           10048576  2005-03-28                    Utility   \n",
       "4           10049016  2004-06-08                    Utility   \n",
       "\n",
       "         examiner_full_name examiner_art_unit uspc_class uspc_subclass  \\\n",
       "0  MITCHELL, LAURA MCGILLEM              1636        435        007400   \n",
       "1               FOX, JOHN C              3753        137        884000   \n",
       "2         SAUCIER, SANDRA E              1651        435        280000   \n",
       "3             FRANCIS, FAYE              3725        241        001000   \n",
       "4               LE, MICHAEL              2163        707        100000   \n",
       "\n",
       "   confirm_number atty_docket_number  \\\n",
       "0          1633.0            01-1637   \n",
       "1          5181.0     442-134 PCT/US   \n",
       "2          4574.0         21581/0286   \n",
       "3          7991.0             020065   \n",
       "4          5734.0         3113.2.1.1   \n",
       "\n",
       "                                    appl_status_desc appl_status_date  \\\n",
       "0  Abandoned -- Failure to Respond to an Office A...       2006-07-10   \n",
       "1  Abandoned -- Failure to Respond to an Office A...       2008-06-09   \n",
       "2  Patent Expired Due to NonPayment of Maintenanc...       2010-09-20   \n",
       "3  Patent Expired Due to NonPayment of Maintenanc...       2011-07-11   \n",
       "4                                      Patented Case       2007-06-13   \n",
       "\n",
       "  file_location file_location_date earliest_pgpub_number earliest_pgpub_date  \\\n",
       "0    ELECTRONIC                NaT       US20050130116A1          2005-06-16   \n",
       "1    ELECTRONIC                NaT       US20050081931A1          2005-04-21   \n",
       "2    ELECTRONIC                NaT       US20050080277A1          2005-04-14   \n",
       "3    ELECTRONIC                NaT       US20050242216A1          2005-11-03   \n",
       "4    ELECTRONIC                NaT       US20050060273A1          2005-03-17   \n",
       "\n",
       "   wipo_pub_number wipo_pub_date patent_number patent_issue_date  \\\n",
       "0              NaN           NaT          None               NaT   \n",
       "1              NaN           NaT          None               NaT   \n",
       "2              NaN           NaT       7094594        2006-08-22   \n",
       "3              NaN           NaT       7229033        2007-06-12   \n",
       "4              NaN           NaT       7240062        2007-07-03   \n",
       "\n",
       "                                     invention_title small_entity_indicator  \\\n",
       "0  Stable expression of polymorphous forms of hum...                  SMALL   \n",
       "1                       Connecting device for fluids           UNDISCOUNTED   \n",
       "2  PROCESS FOR PREPARING OPTICALLY ACTIVE 2-[6-(H...           UNDISCOUNTED   \n",
       "3        METHOD FOR WORKING AND PROCESSING MATERIALS           UNDISCOUNTED   \n",
       "4  SYSTEM AND METHOD FOR CREATING A SEARCHABLE WO...                  SMALL   \n",
       "\n",
       "  aia_first_to_file        publication_number date_application_produced  \\\n",
       "0             false  US20050130116A1-20050616                2005-06-01   \n",
       "1             false  US20050081931A1-20050421                2005-04-06   \n",
       "2             false  US20050080277A1-20050414                2005-03-31   \n",
       "3             false  US20050242216A1-20051103                2005-10-19   \n",
       "4             false  US20050060273A1-20050317                2005-03-03   \n",
       "\n",
       "  date_application_published main_cpc_label cpc_labels main_ipcr_label  \\\n",
       "0                 2005-06-16                                             \n",
       "1                 2005-04-21                                             \n",
       "2                 2005-04-14                                             \n",
       "3                 2005-11-03                                             \n",
       "4                 2005-03-17                                             \n",
       "\n",
       "  ipcr_labels  foreign  continuation  decision decision_as_of_2020  \n",
       "0                 True             0  REJECTED            REJECTED  \n",
       "1                 True             0  REJECTED            REJECTED  \n",
       "2                 True             0  ACCEPTED            ACCEPTED  \n",
       "3                False             0  ACCEPTED            ACCEPTED  \n",
       "4                False             0  ACCEPTED            ACCEPTED  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed28010",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e0cd8",
   "metadata": {},
   "source": [
    "#### Mes enero 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1848b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict = load_dataset('HUPD/hupd',\n",
    "#     name='sample',\n",
    "#     data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "#     icpr_label=None,\n",
    "#     force_extract=True,\n",
    "#     train_filing_start_date='2016-01-01',\n",
    "#     train_filing_end_date='2016-01-21',\n",
    "#     val_filing_start_date='2016-01-22',\n",
    "#     val_filing_end_date='2016-01-31',\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494ab917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e5006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Para el split de entrenamiento:\n",
    "# df_train = pd.DataFrame(dataset_dict['train'])\n",
    "\n",
    "# # Para el split de validación:\n",
    "# df_val = pd.DataFrame(dataset_dict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090e7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.shape, df_val.shape, df_train.shape[0] + df_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b7e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ea16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "604ad739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a584b",
   "metadata": {},
   "source": [
    "#### Año 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36435ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict_2 = load_dataset(\n",
    "#     'HUPD/hupd',\n",
    "#     name='sample',\n",
    "#     icpr_label=None,\n",
    "#     train_filing_start_date='2016-01-01',\n",
    "#     train_filing_end_date='2016-12-31',\n",
    "#     val_filing_start_date='2016-01-01',  # puedes igualar a train si solo quieres un split\n",
    "#     val_filing_end_date='2016-12-31',\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5b1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Para el split de entrenamiento:\n",
    "# df_train = pd.DataFrame(dataset_dict_2['train'])\n",
    "\n",
    "# # Para el split de validación:\n",
    "# df_val = pd.DataFrame(dataset_dict_2['validation'])\n",
    "\n",
    "# print(df_train.shape, df_val.shape, df_train.shape[0] + df_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6c52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"./dataset/hupd_train.csv\", index=False)\n",
    "# df_val.to_csv(\"./dataset/hupd_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13753e",
   "metadata": {},
   "source": [
    "#### Intento años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73cfc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict_3 = load_dataset('HUPD/hupd',\n",
    "#     name='all',\n",
    "#     data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "#     icpr_label=None,\n",
    "#     force_extract=True,\n",
    "#     train_filing_start_date='2015-01-01',\n",
    "#     train_filing_end_date='2015-12-31',\n",
    "#     val_filing_start_date='2017-01-01',\n",
    "#     val_filing_end_date='2017-12-31',\n",
    "# )\n",
    "\n",
    "# print('Loading is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb447143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Para el split de entrenamiento:\n",
    "# df_train = pd.DataFrame(dataset_dict_3['train'])\n",
    "\n",
    "# # Para el split de validación:\n",
    "# df_val = pd.DataFrame(dataset_dict_3['validation'])\n",
    "\n",
    "# print(df_train.shape, df_val.shape, df_train.shape[0] + df_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09abd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"./dataset/hupd_2011_2015.csv\", index=False)\n",
    "# df_val.to_csv(\"./dataset/hupd_2017.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec15336",
   "metadata": {},
   "source": [
    "#### Intento mes y año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533bb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for year in range(2016, 2022):\n",
    "# year = 2017\n",
    "# start = f\"{year}-01-01\"\n",
    "# end = f\"{year}-07-31\"\n",
    "# start_val = f\"{year}-08-01\"\n",
    "# end_val = f\"{year}-12-31\"\n",
    "# dataset = load_dataset(\n",
    "#     'HUPD/hupd',\n",
    "#     name='all',\n",
    "#     data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "#     icpr_label=None,\n",
    "#     force_extract=True,\n",
    "#     train_filing_start_date=start,\n",
    "#     train_filing_end_date=end,\n",
    "#     val_filing_start_date=start_val,\n",
    "#     val_filing_end_date=end_val,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# df_train = pd.DataFrame(dataset['train'])\n",
    "# df_val = pd.DataFrame(dataset['validation'])\n",
    "# df_union = pd.concat([df_train, df_val], ignore_index=True)\n",
    "# df_union.to_csv(f'patentes_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf504991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['decision'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c18f27",
   "metadata": {},
   "source": [
    "### Descarga manual de los archivos y datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872de44",
   "metadata": {},
   "source": [
    "datos descargados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20db2a6",
   "metadata": {},
   "source": [
    "###### funcion de multihilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_chunk(files_chunk, chunk_id):\n",
    "    \"\"\"Procesa un chunk de archivos JSON\"\"\"\n",
    "    print(f\"Hilo {chunk_id}: Iniciando procesamiento de {len(files_chunk)} archivos\")\n",
    "    \n",
    "    data_chunk = []\n",
    "    for i, file in enumerate(files_chunk):\n",
    "        try:\n",
    "            # Leer JSON\n",
    "            with open(file, 'r') as f:\n",
    "                json_val = json.load(f)\n",
    "            \n",
    "            new_dict = {\n",
    "                \"aplication_number\": json_val.get('application_number', ''),\n",
    "                \"publication_number\": json_val.get('publication_number', ''),\n",
    "                \"title\": json_val.get('title', ''),\n",
    "                \"decision\": json_val.get('decision', ''),\n",
    "                \"abstract\": json_val.get('abstract', ''),\n",
    "                \"summary\": json_val.get('summary', ''),\n",
    "                \"description\": json_val.get('full_description', ''),\n",
    "            }\n",
    "            \n",
    "            data_chunk.append(new_dict)\n",
    "            \n",
    "            # # Progreso cada 1000 archivos\n",
    "            # if (i + 1) % 1000 == 0:\n",
    "            #     print(f\"Hilo {chunk_id}: Procesados {i + 1}/{len(files_chunk)} archivos\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Hilo {chunk_id}: Completado! Procesados {len(data_chunk)} archivos exitosamente\")\n",
    "    return data_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37808d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivos en: ./dataset/2014/2014/*.json\n",
      "¿Existe el directorio? True\n",
      "Total de archivos encontrados: 376384\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "year = \"2014\"\n",
    "\n",
    "# Debug del path\n",
    "print(f\"Buscando archivos en: {path_dataset + year + '/' + year + '/' + '*.json'}\")\n",
    "print(f\"¿Existe el directorio? {os.path.exists(path_dataset + year + '/' + year + '/')}\")\n",
    "\n",
    "list_files_json = glob.glob(path_dataset + year + \"/\" + year + \"/\" + \"*.json\")\n",
    "\n",
    "print(f\"Total de archivos encontrados: {len(list_files_json)}\")\n",
    "# 2018 => 7m 27.9s => 31968 # se hizo con Todo en un solo hilo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33132f",
   "metadata": {},
   "source": [
    "###### Todo en varios hilos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_json_copy = list_files_json.copy()\n",
    "versiones = 8\n",
    "len_half_list = int(len(list_files_json) // versiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec32f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunked_files(files,thread_count):\n",
    "    # Dividir la lista en n(thread_count) chunks\n",
    "    chunk_size = len(files) // thread_count\n",
    "    chunks = [\n",
    "        files[:chunk_size],\n",
    "        files[chunk_size:2*chunk_size],\n",
    "        files[2*chunk_size:3*chunk_size],\n",
    "        files[3*chunk_size:4*chunk_size],\n",
    "        files[4*chunk_size:]\n",
    "    ]\n",
    "\n",
    "    print(f\"Chunk 1: {len(chunks[0])} archivos\")\n",
    "    print(f\"Chunk 2: {len(chunks[1])} archivos\") \n",
    "    print(f\"Chunk 3: {len(chunks[2])} archivos\")\n",
    "    print(f\"Chunk 4: {len(chunks[3])} archivos\")\n",
    "    print(f\"Chunk 5: {len(chunks[4])} archivos\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(all_data, version):\n",
    "    # Crear DataFrame final\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        print(\"DataFrame created from JSON.\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        # print(f\"Decision counts:\\n{df['decision'].value_counts()}\")\n",
    "        # print(f\"Columnas: {list(df.keys())}\")\n",
    "        \n",
    "        # Guardar resultado\n",
    "        df.to_csv(f'{path_dataset}patentes_{year}_paralelo_{version}.csv', index=False)\n",
    "        print(f\"Archivo guardado como: patentes_{year}_paralelo_{version}.csv\")\n",
    "    else:\n",
    "        print(\"❌ No se procesaron datos. Verifica la estructura de tus archivos JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 0\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 1.02 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_0.csv\n",
      "Total de registros procesados: 47048 en la versión 0\n",
      "***************************************************************************\n",
      "version: 1\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 2.82 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_1.csv\n",
      "Total de registros procesados: 47048 en la versión 1\n",
      "***************************************************************************\n",
      "version: 2\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 3.34 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_2.csv\n",
      "Total de registros procesados: 47048 en la versión 2\n",
      "***************************************************************************\n",
      "version: 3\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 2.34 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_3.csv\n",
      "Total de registros procesados: 47048 en la versión 3\n",
      "***************************************************************************\n",
      "version: 4\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 1.58 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_4.csv\n",
      "Total de registros procesados: 47048 en la versión 4\n",
      "***************************************************************************\n",
      "version: 5\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 1.58 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_5.csv\n",
      "Total de registros procesados: 47048 en la versión 5\n",
      "***************************************************************************\n",
      "version: 6\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 1.72 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_6.csv\n",
      "Total de registros procesados: 47048 en la versión 6\n",
      "***************************************************************************\n",
      "version: 7\n",
      "Total de archivos a procesar: 47048 de 376384 deberia ser igual a 376384\n",
      "Chunk 1: 9409 archivos\n",
      "Chunk 2: 9409 archivos\n",
      "Chunk 3: 9409 archivos\n",
      "Chunk 4: 9409 archivos\n",
      "Chunk 5: 9412 archivos\n",
      "Hilo 1: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 2: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 3: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 4: Iniciando procesamiento de 9409 archivos\n",
      "Hilo 5: Iniciando procesamiento de 9412 archivos\n",
      "Hilo 3: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 2: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 4: Completado! Procesados 9409 archivos exitosamente\n",
      "Hilo 5: Completado! Procesados 9412 archivos exitosamente\n",
      "Hilo 1: Completado! Procesados 9409 archivos exitosamente\n",
      "\n",
      "Tiempo total de procesamiento: 1.74 minutos\n",
      "DataFrame created from JSON.\n",
      "Shape: (47048, 7)\n",
      "Archivo guardado como: patentes_2014_paralelo_7.csv\n",
      "Total de registros procesados: 47048 en la versión 7\n",
      "***************************************************************************\n",
      "\n",
      "Tiempo total de procesamiento: 1.74 minutos\n",
      "Total de registros procesados: 376384\n"
     ]
    }
   ],
   "source": [
    "sum_row_df = 0\n",
    "for version in range(versiones):\n",
    "    if version == versiones - 1:\n",
    "        # Último chunk toma el resto de los archivos\n",
    "        list_files_json = list_files_json_copy[version * len_half_list:]\n",
    "    else:\n",
    "        list_files_json = list_files_json_copy[version * len_half_list:(version + 1) * len_half_list]\n",
    "\n",
    "    print(f\"version: {version}\")\n",
    "    print(f\"Total de archivos a procesar: {len(list_files_json)} de {len(list_files_json) * versiones} deberia ser igual a {len(list_files_json_copy)}\")\n",
    "\n",
    "    thread_count = 5\n",
    "    # Chunks\n",
    "    chunks = get_chunked_files(list_files_json, thread_count=thread_count)\n",
    "\n",
    "    # Procesar en paralelo\n",
    "    start_time = time.time()\n",
    "    all_data = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:\n",
    "        # Enviar tareas a los hilos\n",
    "        futures = [\n",
    "            executor.submit(process_files_chunk, chunks[i], i+1) \n",
    "            for i in range(thread_count)\n",
    "        ]\n",
    "        \n",
    "        # Recoger resultados\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                chunk_data = future.result()\n",
    "                sum_row_df += len(chunk_data)\n",
    "                all_data.extend(chunk_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error en hilo: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTiempo total de procesamiento: {(end_time - start_time)/60:.2f} minutos\")\n",
    "    save_dataframe(all_data, version)\n",
    "    print(f\"Total de registros procesados: {len(all_data)} en la versión {version}\")\n",
    "    print(\"*\"*75)\n",
    "print(f\"\\nTiempo total de procesamiento: {(end_time - start_time)/60:.2f} minutos\")\n",
    "print(f\"Total de registros procesados: {sum_row_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc836687",
   "metadata": {},
   "source": [
    "###### Todo en varios hilos y con diferentes versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05a0132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # version = 1\n",
    "# # list_files_json = list_files_json_copy[:len_half_list]\n",
    "\n",
    "# # version = 2\n",
    "# # list_files_json = list_files_json_copy[len_half_list : 2*len_half_list]\n",
    "\n",
    "# # version = 3\n",
    "# # list_files_json = list_files_json_copy[2*len_half_list:3*len_half_list]\n",
    "\n",
    "# # version = 4\n",
    "# # list_files_json = list_files_json_copy[3*len_half_list: 4*len_half_list]\n",
    "\n",
    "# # version = 5\n",
    "# # list_files_json = list_files_json_copy[4*len_half_list: 5*len_half_list]\n",
    "\n",
    "# # version = 6\n",
    "# # list_files_json = list_files_json_copy[5*len_half_list: 6*len_half_list]\n",
    "\n",
    "# # version = 7\n",
    "# # list_files_json = list_files_json_copy[6*len_half_list: 7*len_half_list]\n",
    "\n",
    "# version = 8\n",
    "# list_files_json = list_files_json_copy[7*len_half_list:]\n",
    "\n",
    "# print(f\"version: {version}\")\n",
    "# print(f\"Total de archivos a procesar: {len(list_files_json)} de {len(list_files_json) * versiones} deberia ser igual a {len(list_files_json_copy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3722f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dividir la lista en 3 chunks\n",
    "# thread_count = 5\n",
    "# chunk_size = len(list_files_json) // thread_count\n",
    "# chunks = [\n",
    "#     list_files_json[:chunk_size],\n",
    "#     list_files_json[chunk_size:2*chunk_size],\n",
    "#     list_files_json[2*chunk_size:3*chunk_size],\n",
    "#     list_files_json[3*chunk_size:4*chunk_size],\n",
    "#     list_files_json[4*chunk_size:]\n",
    "# ]\n",
    "\n",
    "# print(f\"Chunk 1: {len(chunks[0])} archivos\")\n",
    "# print(f\"Chunk 2: {len(chunks[1])} archivos\") \n",
    "# print(f\"Chunk 3: {len(chunks[2])} archivos\")\n",
    "# print(f\"Chunk 4: {len(chunks[3])} archivos\")\n",
    "# print(f\"Chunk 5: {len(chunks[4])} archivos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b9a01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Procesar en paralelo\n",
    "# start_time = time.time()\n",
    "# all_data = []\n",
    "# ## ----------------------------------\n",
    "# # thread_results = {}  # Diccionario para almacenar resultados por hilo\n",
    "# ## ----------------------------------\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:\n",
    "#     # Enviar tareas a los hilos\n",
    "#     futures = [\n",
    "#         executor.submit(process_files_chunk, chunks[i], i+1) \n",
    "#         for i in range(thread_count)\n",
    "#     ]\n",
    "    \n",
    "#     # Recoger resultados\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         ## ----------------------------------\n",
    "#         # thread_id = futures[future]\n",
    "#         ## ----------------------------------\n",
    "#         try:\n",
    "#             chunk_data = future.result()\n",
    "#             all_data.extend(chunk_data)\n",
    "\n",
    "#             ## ----------------------------------\n",
    "#             # thread_results[thread_id] = chunk_data\n",
    "#             # print(f\"✅ Hilo {thread_id} completado: {len(chunk_data)} registros procesados\")\n",
    "#             ## ----------------------------------\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error en hilo: {e}\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"\\nTiempo total de procesamiento: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82426844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ----------------------------------\n",
    "# # Combinar todos los resultados\n",
    "# all_data = []\n",
    "# for thread_id in sorted(thread_results.keys()):\n",
    "#     thread_data = thread_results[thread_id]\n",
    "#     all_data.extend(thread_data)\n",
    "#     print(f\"Hilo {thread_id}: {len(thread_data)} registros añadidos al resultado final\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"\\nTiempo total de procesamiento: {end_time - start_time:.2f} segundos\")\n",
    "# print(f\"Total de registros procesados: {len(all_data)}\")\n",
    "\n",
    "# ## ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1891f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear DataFrame final\n",
    "# if all_data:\n",
    "#     df = pd.DataFrame(all_data)\n",
    "#     print(\"DataFrame created from JSON.\")\n",
    "#     print(f\"Shape: {df.shape}\")\n",
    "#     # print(f\"Decision counts:\\n{df['decision'].value_counts()}\")\n",
    "#     # print(f\"Columnas: {list(df.keys())}\")\n",
    "    \n",
    "#     # Guardar resultado\n",
    "#     df.to_csv(f'{path_dataset}patentes_{year}_paralelo_{version}.csv', index=False)\n",
    "#     print(f\"Archivo guardado como: patentes_{year}_paralelo_{version}.csv\")\n",
    "# else:\n",
    "#     print(\"❌ No se procesaron datos. Verifica la estructura de tus archivos JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee5782",
   "metadata": {},
   "source": [
    "###### Todo en un solo hilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89ab5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# item = 0\n",
    "# for file in list_files_json:\n",
    "#     print(file)\n",
    "#     # read json \n",
    "#     print(\"Reading JSON file...\")\n",
    "#     json_val = json.load(open(file, 'r'))\n",
    "#     new_dict = {\n",
    "#         \"aplication_number\": json_val['application_number'],\n",
    "#         \"publication_number\": json_val['publication_number'],\n",
    "#         \"title\": json_val['title'],\n",
    "#         \"decision\": json_val['decision'],\n",
    "#         \"abstract\": json_val['abstract'],\n",
    "#         \"summary\": json_val['summary'],\n",
    "#         \"description\": json_val['full_description'],\n",
    "#     }\n",
    "#     # print(\"JSON loaded successfully.\")\n",
    "#     # print(f\"Number of records: {len(new_dict)}\")\n",
    "#     # print(f\"Keys in JSON: {new_dict.keys() if new_dict else 'No records found'}\")\n",
    "#     print(\"Item:\", item)\n",
    "#     item += 1\n",
    "#     data.append(new_dict)\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# print(\"DataFrame created from JSON.\")\n",
    "# print(df.shape)\n",
    "# print(df['decision'].value_counts())\n",
    "# print(df.keys())\n",
    "\n",
    "# print(\"Saving DataFrame to CSV '\" + path_dataset + year + \".csv'\" + \"...\")\n",
    "# df.to_csv(path_dataset + year + \".csv\", index=False)\n",
    "# print(\"Saved to CSV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
